{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall Detection Prediction from Video \n",
    "\n",
    "Loads a pre-trained Fall Detection RNN model and predicts the action in an input video.\n",
    "\n",
    "**Configuration MUST match the model's training parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import argparse # Removed for notebook usage\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf')\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")\n",
    "print(f\"Mediapipe Version: {mp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration (MUST MATCH TRAINING CONFIGURATION of model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Architecture Parameters ---\n",
    "# !! IMPORTANT: These MUST match the parameters used to train the saved model !!\n",
    "INPUT_SIZE = 33 * 4  # 132 (33 landmarks * [norm_x, norm_y, vel_x, vel_y])\n",
    "HIDDEN_SIZE = 192\n",
    "NUM_LAYERS = 2\n",
    "NUM_CLASSES = 4      # Number of classes model was trained on\n",
    "RNN_TYPE = 'LSTM'    # Or 'GRU', depending on the saved model\n",
    "DROPOUT_PROB = 0.4   # Must match the saved model training parameter\n",
    "BIDIRECTIONAL = True # Must match the saved model training parameter\n",
    "\n",
    "# --- Preprocessing Parameters ---\n",
    "# !! IMPORTANT: These MUST match the parameters used during feature extraction/training !!\n",
    "FRAME_SKIP = 3 # How often frames were sampled for the pkl files (adjust if different)\n",
    "SEQUENCE_LENGTH = 30 # Original sequence length from training\n",
    "ORIGINAL_LANDMARK_DIM = 3 # x, y, visibility from mediapipe\n",
    "NUM_LANDMARKS = 33 # Mediapipe Pose\n",
    "\n",
    "# --- Class Labels ---\n",
    "# !! Make sure this order/mapping matches the training !!\n",
    "CLASS_NAMES = [\"backward_fall\", \"forward_fall\", \"side_fall\", \"non_fall\"]\n",
    "index_to_name = {i: name for i, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# --- Device Configuration ---\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# --- Hip landmark indices (needed for normalization) ---\n",
    "LEFT_HIP_IDX = 23\n",
    "RIGHT_HIP_IDX = 24\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Model Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FallDetectionRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, rnn_type='LSTM', dropout_prob=0.2, bidirectional=True):\n",
    "        super(FallDetectionRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        rnn_dropout = dropout_prob if num_layers > 1 else 0\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                               batch_first=True, dropout=rnn_dropout,\n",
    "                               bidirectional=bidirectional)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                              batch_first=True, dropout=rnn_dropout,\n",
    "                              bidirectional=bidirectional)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Input to FC layer is hidden_size * num_directions\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        # Initialize hidden state for bidirectional\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(x.device)\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            c0 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(x.device)\n",
    "            hidden = (h0, c0)\n",
    "        else: # GRU\n",
    "            hidden = h0\n",
    "\n",
    "        # RNN output shape: (batch, seq_len, num_directions * hidden_size)\n",
    "        # hidden state shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, _ = self.rnn(x, hidden)\n",
    "\n",
    "        # Use the output of the last time step\n",
    "        # This contains the concatenation of the last forward and backward hidden states\n",
    "        last_step_out = out[:, -1, :]\n",
    "        last_step_out = self.dropout(last_step_out)\n",
    "        out = self.fc(last_step_out)\n",
    "        return out\n",
    "\n",
    "print(\"Model class defined.\") # Added minimal print confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Feature Extraction Functions (Normalization & Velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_and_calculate_velocity(sequence_data_raw, sequence_length, input_size, num_landmarks, orig_landmark_dim):\n",
    "    \"\"\"\n",
    "    Normalizes pose relative to hip center, calculates velocity, and combines them.\n",
    "    Mirrors the logic from the training Dataset.\n",
    "\n",
    "    Args:\n",
    "        sequence_data_raw (np.ndarray): Raw landmark data, shape (extracted_frames, num_landmarks * orig_dim)\n",
    "                                        or (extracted_frames, num_landmarks, orig_dim).\n",
    "        sequence_length (int): Target sequence length (e.g., 30).\n",
    "        input_size (int): Final input size for the model (e.g., 132).\n",
    "        num_landmarks (int): Number of landmarks (e.g., 33).\n",
    "        orig_landmark_dim (int): Dimensions per landmark in raw data (e.g., 3 for x, y, vis).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Processed sequence, shape (sequence_length, input_size), or None if error.\n",
    "    \"\"\"\n",
    "    num_extracted_frames = sequence_data_raw.shape[0]\n",
    "    if num_extracted_frames == 0:\n",
    "        print(\"Warning: _normalize_and_calculate_velocity received empty raw sequence.\")\n",
    "        return np.zeros((sequence_length, input_size), dtype=np.float32) # Return zeros\n",
    "\n",
    "    # --- Reshape raw data if needed ---\n",
    "    # Expected input shape for processing: (num_extracted_frames, num_landmarks, orig_dim)\n",
    "    try:\n",
    "        if sequence_data_raw.ndim == 2: # If it's flattened (frames, landmarks * dim)\n",
    "             sequence_reshaped_raw = sequence_data_raw.reshape(\n",
    "                 num_extracted_frames, num_landmarks, orig_landmark_dim\n",
    "             )\n",
    "        elif sequence_data_raw.ndim == 3: # If it's already (frames, landmarks, dim)\n",
    "            sequence_reshaped_raw = sequence_data_raw\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected raw data shape: {sequence_data_raw.shape}\")\n",
    "    except ValueError as e:\n",
    "         print(f\"Error reshaping raw sequence data. Shape: {sequence_data_raw.shape}. Error: {e}\")\n",
    "         return np.zeros((sequence_length, input_size), dtype=np.float32) # Return zeros\n",
    "\n",
    "    # --- Pad or Truncate the reshaped raw sequence to SEQUENCE_LENGTH ---\n",
    "    padded_sequence_raw = np.zeros((sequence_length, num_landmarks, orig_landmark_dim), dtype=np.float32)\n",
    "    len_to_copy = min(num_extracted_frames, sequence_length)\n",
    "    padded_sequence_raw[:len_to_copy] = sequence_reshaped_raw[:len_to_copy]\n",
    "    # Note: Padding remaining frames with zeros is implicit here.\n",
    "\n",
    "    # --- Calculate Normalized Coordinates and Velocity ---\n",
    "    normalized_coords = np.zeros((sequence_length, num_landmarks, 2), dtype=np.float32) # Store norm_x, norm_y\n",
    "    velocities = np.zeros((sequence_length, num_landmarks, 2), dtype=np.float32) # Store vel_x, vel_y\n",
    "    last_norm_coords = None\n",
    "    last_center_x, last_center_y = 0.0, 0.0 # Initialize for velocity check\n",
    "\n",
    "    for t in range(sequence_length):\n",
    "        frame_data = padded_sequence_raw[t] # (num_landmarks, orig_dim)\n",
    "\n",
    "        # Calculate hip center for normalization\n",
    "        left_hip = frame_data[LEFT_HIP_IDX, :2] # x, y\n",
    "        right_hip = frame_data[RIGHT_HIP_IDX, :2] # x, y\n",
    "\n",
    "        # Basic check (using visibility if available, otherwise just check coords)\n",
    "        # A more robust check might be needed depending on data quality\n",
    "        if np.all(left_hip == 0) and np.all(right_hip == 0): # Use and, not or\n",
    "            center_x, center_y = 0.0, 0.0 # Fallback - consider previous frame's center if needed\n",
    "        else:\n",
    "             center_x = (left_hip[0] + right_hip[0]) / 2.0\n",
    "             center_y = (left_hip[1] + right_hip[1]) / 2.0\n",
    "\n",
    "        # Normalize coordinates (subtract center)\n",
    "        current_norm_coords = frame_data[:, :2] - np.array([center_x, center_y])\n",
    "        normalized_coords[t] = current_norm_coords\n",
    "\n",
    "        # Calculate velocity (difference from last frame's normalized coords)\n",
    "        if last_norm_coords is not None:\n",
    "             # Avoid calculating velocity if current or previous frame was likely padding (all coords == -center)\n",
    "            current_is_padding = np.allclose(current_norm_coords, -np.array([center_x, center_y]))\n",
    "            last_is_padding = np.allclose(last_norm_coords, -np.array([last_center_x, last_center_y]))\n",
    "            \n",
    "            if not current_is_padding and not last_is_padding:\n",
    "                 velocities[t] = current_norm_coords - last_norm_coords\n",
    "             # else: velocity remains zero\n",
    "        # else: velocity remains zero for the first frame\n",
    "\n",
    "        last_norm_coords = current_norm_coords.copy() # Use copy to prevent mutation issues\n",
    "        last_center_x, last_center_y = center_x, center_y # Store center used for next frame's velocity check\n",
    "\n",
    "\n",
    "    # Combine features: [norm_x, norm_y, vel_x, vel_y] -> shape: (seq_len, num_landmarks, 4)\n",
    "    combined_features = np.concatenate((normalized_coords, velocities), axis=-1)\n",
    "\n",
    "    # Reshape back to (seq_len, num_landmarks * 4) which is (seq_len, input_size)\n",
    "    try:\n",
    "        final_processed_sequence = combined_features.reshape(sequence_length, input_size)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reshaping combined features: {e}. Target shape: ({sequence_length}, {input_size}), Actual combined shape: {combined_features.shape}\")\n",
    "        return np.zeros((sequence_length, input_size), dtype=np.float32) # Return zeros on error\n",
    "\n",
    "    return final_processed_sequence\n",
    "\n",
    "\n",
    "def extract_and_process_features_from_video(video_path, frame_skip, sequence_length, input_size, num_landmarks, orig_landmark_dim):\n",
    "    \"\"\"\n",
    "    Extracts Mediapipe keypoints, handles sequence length, normalizes,\n",
    "    calculates velocity, and formats for the V2 model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array of shape (sequence_length, input_size)\n",
    "                    containing processed features, or None if processing fails.\n",
    "    \"\"\"\n",
    "    # Initialize Mediapipe Pose\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = None # Initialize to None\n",
    "    cap = None # Initialize to None\n",
    "\n",
    "    try:\n",
    "        pose = mp_pose.Pose(static_image_mode=False, model_complexity=1,\n",
    "                            smooth_landmarks=True, enable_segmentation=False,\n",
    "                            min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video {video_path}\")\n",
    "            # pose object might not exist yet if VideoCapture failed early\n",
    "            if pose: pose.close()\n",
    "            return None\n",
    "\n",
    "        frame_count = 0\n",
    "        raw_keypoints_sequence = [] # Store raw landmark arrays temporarily\n",
    "        frames_extracted_count = 0\n",
    "\n",
    "        # --- Step 1: Extract Raw Keypoints from Video Frames ---\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break # End of video\n",
    "\n",
    "            # Process only every Nth frame\n",
    "            if frame_count % frame_skip == 0:\n",
    "                frames_extracted_count += 1 # Increment count for sampled frame\n",
    "                try:\n",
    "                    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    image_rgb.flags.writeable = False\n",
    "                    results = pose.process(image_rgb)\n",
    "\n",
    "                    if results.pose_landmarks:\n",
    "                        frame_keypoints = []\n",
    "                        for landmark in results.pose_landmarks.landmark:\n",
    "                            # Store x, y, visibility for each landmark\n",
    "                            frame_keypoints.append([landmark.x, landmark.y, landmark.visibility])\n",
    "                        # Append as shape (num_landmarks, orig_landmark_dim)\n",
    "                        raw_keypoints_sequence.append(np.array(frame_keypoints, dtype=np.float32))\n",
    "                    else:\n",
    "                        # Pad with zeros if no pose detected in a sampled frame\n",
    "                        raw_keypoints_sequence.append(np.zeros((num_landmarks, orig_landmark_dim), dtype=np.float32))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing frame {frame_count} in {video_path}: {e}\")\n",
    "                    # Append zeros if error occurs during processing\n",
    "                    raw_keypoints_sequence.append(np.zeros((num_landmarks, orig_landmark_dim), dtype=np.float32))\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # Optimization: Stop slightly after gathering enough *sampled* frames for the sequence\n",
    "            if frames_extracted_count > sequence_length + 5: # Add a small buffer\n",
    "                 break\n",
    "\n",
    "        # Convert list of arrays to a single numpy array (extracted_frames, num_landmarks, orig_dim)\n",
    "        if not raw_keypoints_sequence:\n",
    "             print(f\"Warning: No keypoints extracted from {video_path}\")\n",
    "             # Ensure resources are released even if no keypoints found\n",
    "             if cap: cap.release()\n",
    "             if pose: pose.close()\n",
    "             return None\n",
    "             \n",
    "        sequence_data_raw = np.stack(raw_keypoints_sequence, axis=0)\n",
    "\n",
    "        # --- Step 2: Normalize, Calculate Velocity, Pad/Truncate ---\n",
    "        processed_sequence = _normalize_and_calculate_velocity(\n",
    "            sequence_data_raw,\n",
    "            sequence_length,\n",
    "            input_size,\n",
    "            num_landmarks,\n",
    "            orig_landmark_dim\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in extract_and_process_features_from_video: {e}\")\n",
    "        processed_sequence = None # Ensure return value is None on error\n",
    "        # traceback.print_exc() # Uncomment for detailed debug trace\n",
    "    finally:\n",
    "        # --- Release Resources ---\n",
    "        if cap: \n",
    "            cap.release()\n",
    "        if pose: \n",
    "            pose.close()\n",
    "\n",
    "    return processed_sequence\n",
    "\n",
    "print(\"Feature extraction functions defined.\") # Added minimal print confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Main Prediction Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_video(video_path, model_path):\n",
    "    \"\"\"\n",
    "    Loads the model, processes a video using the v2 feature pipeline,\n",
    "    and returns the prediction.\n",
    "    \"\"\"\n",
    "    # --- Validate inputs ---\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file not found at '{video_path}'\")\n",
    "        return None, None # Return tuple for consistency\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file not found at '{model_path}'\")\n",
    "        return None, None # Return tuple for consistency\n",
    "\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "    # --- Load Model ---\n",
    "    model = None # Initialize\n",
    "    try:\n",
    "        # 1. Instantiate the model with the *exact same* parameters as v2 training\n",
    "        model = FallDetectionRNN(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            rnn_type=RNN_TYPE,\n",
    "            dropout_prob=DROPOUT_PROB, # Use dropout from config\n",
    "            bidirectional=BIDIRECTIONAL # Use bidirectional flag from config\n",
    "        )\n",
    "\n",
    "        # 2. Load the saved state dictionary\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "\n",
    "        # 3. Move model to the appropriate device\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        # 4. Set model to evaluation mode\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- Preprocess Video (using the function from Cell 4) ---\n",
    "    print(f\"\\nProcessing video with v2 pipeline: {video_path}\")\n",
    "    start_time = time.time()\n",
    "    processed_feature_data = extract_and_process_features_from_video(\n",
    "        video_path,\n",
    "        FRAME_SKIP,\n",
    "        SEQUENCE_LENGTH,\n",
    "        INPUT_SIZE,\n",
    "        NUM_LANDMARKS,\n",
    "        ORIGINAL_LANDMARK_DIM\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    if processed_feature_data is None:\n",
    "        print(\"Failed to extract and process features from the video.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Feature extraction & processing took {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Processed feature shape: {processed_feature_data.shape}\") # Should be (SEQUENCE_LENGTH, INPUT_SIZE)\n",
    "\n",
    "    # --- Prepare Tensor ---\n",
    "    # Add batch dimension (batch size of 1) and move to device\n",
    "    try:\n",
    "        sequence_tensor = torch.tensor(processed_feature_data, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating tensor from processed data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- Perform Inference ---\n",
    "    print(\"Running inference...\")\n",
    "    predicted_class_name = None\n",
    "    prediction_confidence = None\n",
    "    try:\n",
    "        with torch.no_grad(): # Disable gradient calculations for inference\n",
    "            outputs = model(sequence_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "\n",
    "            predicted_class_index = predicted_idx.item()\n",
    "            prediction_confidence = confidence.item()\n",
    "\n",
    "        # --- Map index to class name ---\n",
    "        predicted_class_name = index_to_name.get(predicted_class_index, f\"Unknown Index {predicted_class_index}\")\n",
    "\n",
    "        print(\"\\n--- Prediction Result ---\")\n",
    "        print(f\"  Detected Class: {predicted_class_name}\")\n",
    "        print(f\"  Confidence: {prediction_confidence:.4f}\")\n",
    "        print(\"  Class Probabilities:\")\n",
    "        # Ensure probabilities are on CPU for printing\n",
    "        probabilities_cpu = probabilities.squeeze().cpu().numpy()\n",
    "        for i, name in index_to_name.items():\n",
    "            prob = probabilities_cpu[i] if i < len(probabilities_cpu) else -1 # Safety\n",
    "            print(f\"    {name}: {prob:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model inference: {e}\")\n",
    "        # traceback.print_exc() # Uncomment for detailed debug trace\n",
    "        return None, None\n",
    "\n",
    "    return predicted_class_name, prediction_confidence\n",
    "\n",
    "print(\"Prediction function defined.\") # Added minimal print confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Script Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell replaces the `if __name__ == \"__main__\":` block for notebook usage.\n",
    "\n",
    "# <<< --- USER INPUT: Define paths here --- >>>\n",
    "\n",
    "# 1. Specify the full path to the video file you want to predict.\n",
    "video_to_predict_path = '/Users/samnangpheng/Desktop/Fall_detection/dataset/raw/non_fall/00288_H_A_N_C4.mp4' # <-- CHANGE THIS\n",
    "\n",
    "# 2. Specify the full path to the trained V2 model file (.pth).\n",
    "model_file_path = '/Users/samnangpheng/Desktop/Fall_detection/trained_models_local/fall_detection_rnn_local_tuned.pth' # <-- CHANGE THIS\n",
    "\n",
    "# <<< --- End of User Input --- >>>\n",
    "\n",
    "# --- Run Prediction ---\n",
    "print(f\"Starting prediction for video: {video_to_predict_path}\")\n",
    "print(f\"Using model: {model_file_path}\")\n",
    "\n",
    "# Call the main prediction function\n",
    "predicted_class, confidence = predict_single_video(video_to_predict_path, model_file_path)\n",
    "\n",
    "# --- Final Output --- \n",
    "print(\"\\n--- Prediction Finished --- \")\n",
    "if predicted_class is not None:\n",
    "    print(f\"Final Result for '{os.path.basename(video_to_predict_path)}':\")\n",
    "    print(f\"  Predicted Action: {predicted_class}\")\n",
    "    print(f\"  Confidence Score: {confidence:.4f}\")\n",
    "else:\n",
    "    print(f\"Prediction failed for '{os.path.basename(video_to_predict_path)}'. Check error messages above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
