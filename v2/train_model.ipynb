{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall Detection RNN (Weighted Loss Attempt)\n",
    "\n",
    "This notebook trains a Recurrent Neural Network (RNN) using LSTM or GRU cells to detect falls from pose estimation data. This version specifically incorporates:\n",
    "*   **Weighted Cross-Entropy Loss:** To address potential class imbalance.\n",
    "*   **Feature Engineering:** Normalization relative to hip center and velocity calculation.\n",
    "*   **Data Augmentation:** Adding noise during training.\n",
    "*   **Bidirectional RNN:** To capture context from both past and future frames.\n",
    "*   **Hyperparameter Tuning:** Adjustments to learning rate, dropout, patience, etc.\n",
    "*   **Early Stopping & LR Scheduling:** For efficient training.\n",
    "*   **Detailed Evaluation:** Including confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import random # For augmentation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report # For detailed evaluation\n",
    "import seaborn as sns # For plotting confusion matrix\n",
    "import matplotlib.pyplot as plt # For plotting confusion matrix and accuracy graph\n",
    "import pandas as pd # For confusion matrix dataframe\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import Counter \n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Configuration - Updated for Weighted Loss & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User Defined Paths ---\n",
    "# <<< IMPORTANT: ADJUST THIS PATH TO YOUR PROJECT LOCATION >>>\n",
    "BASE_PROJECT_PATH = '/Users/samnangpheng/Desktop/Fall_detection' # <-- ADJUST THIS PATH!!!\n",
    "\n",
    "FEATURES_DIR = os.path.join(BASE_PROJECT_PATH, 'dataset/processed_features')\n",
    "MODEL_SAVE_DIR = os.path.join(BASE_PROJECT_PATH, 'trained_models_local')\n",
    "# Give the new model a distinct name\n",
    "MODEL_NAME = 'fall_detection_rnn_local_weighted_loss.pth' \n",
    "\n",
    "CLASS_FOLDERS = [\"backward_fall\", \"forward_fall\", \"side_fall\", \"non_fall\"]\n",
    "\n",
    "# --- Feature & Preprocessing Parameters ---\n",
    "ORIGINAL_SEQUENCE_LENGTH = 30\n",
    "ORIGINAL_LANDMARK_DIM = 3\n",
    "INPUT_SIZE = 33 * 4 # 132 (33 landmarks * [norm_x, norm_y, vel_x, vel_y])\n",
    "\n",
    "# --- Data Augmentation Parameters (for Training) ---\n",
    "AUGMENT_PROB = 0.55 \n",
    "NOISE_LEVEL = 0.006 \n",
    "\n",
    "# --- Model & Training Hyperparameters ---\n",
    "NUM_CLASSES = len(CLASS_FOLDERS)\n",
    "HIDDEN_SIZE = 192      \n",
    "NUM_LAYERS = 2         \n",
    "RNN_TYPE = 'LSTM'      \n",
    "BIDIRECTIONAL = True   \n",
    "\n",
    "# --- Training Strategy Parameters ---\n",
    "USE_WEIGHTED_LOSS = True\n",
    "\n",
    "DROPOUT_PROB = 0.35    \n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0004 \n",
    "WEIGHT_DECAY = 1e-5   \n",
    "NUM_EPOCHS = 150       \n",
    "VALIDATION_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "PATIENCE_EARLY_STOPPING = 25 \n",
    "PATIENCE_LR_SCHEDULER = 10   \n",
    "\n",
    "# --- Device Configuration ---\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Create output directories ---\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Class Mappings ---\n",
    "label_map = {name: i for i, name in enumerate(CLASS_FOLDERS)}\n",
    "index_to_name = {i: name for name, i in label_map.items()}\n",
    "\n",
    "print(\"\\nConfiguration loaded (Weighted Loss Attempt):\")\n",
    "print(f\"  Input Size: {INPUT_SIZE}\")\n",
    "print(f\"  Hidden Size: {HIDDEN_SIZE}, Num Layers: {NUM_LAYERS}, Bidirectional: {BIDIRECTIONAL}\")\n",
    "print(f\"  Use Weighted Loss: {USE_WEIGHTED_LOSS}\") \n",
    "print(f\"  Dropout: {DROPOUT_PROB}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}, Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Augmentation Prob: {AUGMENT_PROB}, Noise Level: {NOISE_LEVEL}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}, Early Stopping Patience: {PATIENCE_EARLY_STOPPING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Data Loading & Splitting (Reads existing .pkl files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_files(feature_dir, class_folders, label_map):\n",
    "    all_feature_files = []\n",
    "    all_labels = []\n",
    "    print(f\"\\nLoading feature files from: {feature_dir}\")\n",
    "    if not os.path.isdir(feature_dir): raise FileNotFoundError(f\"Feature directory not found: {feature_dir}\")\n",
    "    for class_name in class_folders:\n",
    "        class_label = label_map.get(class_name)\n",
    "        if class_label is None: print(f\"Warning: Class '{class_name}' not found in label_map. Skipping.\"); continue\n",
    "        class_path = os.path.join(feature_dir, class_name)\n",
    "        if not os.path.isdir(class_path): print(f\"Warning: Class directory not found: {class_path}. Skipping.\"); continue\n",
    "        pkl_files = glob.glob(os.path.join(class_path, '*.pkl'))\n",
    "        if not pkl_files: print(f\"Warning: No .pkl files found in {class_path}\"); continue\n",
    "        print(f\"  Found {len(pkl_files)} features for class: {class_name}\")\n",
    "        all_feature_files.extend(pkl_files)\n",
    "        all_labels.extend([class_label] * len(pkl_files))\n",
    "    if not all_feature_files: raise ValueError(f\"No .pkl files found in any class subdirectories of {feature_dir}\")\n",
    "    print(f\"\\nTotal feature files found: {len(all_feature_files)}\")\n",
    "    return all_feature_files, np.array(all_labels)\n",
    "\n",
    "try:\n",
    "    all_files, all_labels = load_feature_files(FEATURES_DIR, CLASS_FOLDERS, label_map)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading feature files: {e}\"); exit()\n",
    "\n",
    "# --- Split Data: Train / Validation / Test ---\n",
    "train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "    all_files, all_labels, test_size=TEST_SPLIT, random_state=42, stratify=all_labels)\n",
    "val_split_adjusted = VALIDATION_SPLIT / (1.0 - TEST_SPLIT)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_val_files, train_val_labels, test_size=val_split_adjusted, random_state=42, stratify=train_val_labels)\n",
    "\n",
    "print(\"\\nData Splitting Complete:\")\n",
    "print(f\"  Training samples:   {len(train_files)}\")\n",
    "print(f\"  Validation samples: {len(val_files)}\")\n",
    "print(f\"  Testing samples:    {len(test_files)}\")\n",
    "if not train_files or not val_files or not test_files:\n",
    "    print(\"Warning: One or more data splits are empty.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Dataset with Feature Engineering & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to load .pkl files, perform feature engineering\n",
    "    (normalization, velocity), and optional augmentation (noise).\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_paths, labels, sequence_length, input_size,\n",
    "                 orig_landmark_dim=3, is_train=False, augment_prob=0.0, noise_level=0.0):\n",
    "        self.feature_paths = feature_paths\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length \n",
    "        self.input_size = input_size \n",
    "        self.orig_landmark_dim = orig_landmark_dim\n",
    "        self.num_landmarks = 33 \n",
    "\n",
    "        # Augmentation parameters (only used if is_train is True)\n",
    "        self.is_train = is_train\n",
    "        self.augment_prob = augment_prob\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "        # Define hip landmark indices\n",
    "        self.left_hip_idx = 23\n",
    "        self.right_hip_idx = 24\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_paths)\n",
    "\n",
    "    def _normalize_and_calculate_velocity(self, sequence_data_raw):\n",
    "        \"\"\"Normalizes pose, calculates velocity, and combines them.\"\"\"\n",
    "        current_path = \"Unknown\" # Placeholder in case idx isn't available in exception handling\n",
    "        try:\n",
    "            # Ensure raw data is numpy array\n",
    "            if isinstance(sequence_data_raw, torch.Tensor):\n",
    "                sequence_data_raw = sequence_data_raw.numpy()\n",
    "            # Ensure raw data has the expected flat shape before reshaping\n",
    "            expected_flat_len = self.sequence_length * self.num_landmarks * self.orig_landmark_dim\n",
    "            if sequence_data_raw.size != expected_flat_len:\n",
    "                 print(f\"Warning: Raw data size mismatch. Expected {expected_flat_len}, got {sequence_data_raw.size}. Path: {current_path}. Returning zeros.\")\n",
    "                 return np.zeros((self.sequence_length, self.input_size), dtype=np.float32)\n",
    "\n",
    "            sequence_reshaped = sequence_data_raw.reshape(\n",
    "                self.sequence_length, self.num_landmarks, self.orig_landmark_dim\n",
    "            )\n",
    "        except ValueError as e:\n",
    "             print(f\"Error reshaping sequence data. Expected flat length {expected_flat_len}, got shape {sequence_data_raw.shape}. Error: {e}. Path: {current_path}\")\n",
    "             return np.zeros((self.sequence_length, self.input_size), dtype=np.float32)\n",
    "        except AttributeError as e: # Handle cases where it might not be a numpy array initially\n",
    "             print(f\"Attribute error during reshape (is data a numpy array?). Shape: {getattr(sequence_data_raw, 'shape', 'N/A')}. Error: {e}. Path: {current_path}\")\n",
    "             return np.zeros((self.sequence_length, self.input_size), dtype=np.float32)\n",
    "\n",
    "\n",
    "        normalized_coords = np.zeros((self.sequence_length, self.num_landmarks, 2), dtype=np.float32) # Store norm_x, norm_y\n",
    "        velocities = np.zeros((self.sequence_length, self.num_landmarks, 2), dtype=np.float32) # Store vel_x, vel_y\n",
    "        last_norm_coords = None\n",
    "\n",
    "        for t in range(self.sequence_length):\n",
    "            frame_data = sequence_reshaped[t]\n",
    "             # Basic check for valid hip indices\n",
    "            if self.left_hip_idx >= frame_data.shape[0] or self.right_hip_idx >= frame_data.shape[0]:\n",
    "                 print(f\"Warning: Invalid hip indices {self.left_hip_idx}, {self.right_hip_idx} for frame shape {frame_data.shape}. Skipping frame {t} normalization. Path: {current_path}\")\n",
    "                 continue \n",
    "\n",
    "            left_hip = frame_data[self.left_hip_idx, :2]\n",
    "            right_hip = frame_data[self.right_hip_idx, :2]\n",
    "            center_x = (left_hip[0] + right_hip[0]) / 2.0\n",
    "            center_y = (left_hip[1] + right_hip[1]) / 2.0\n",
    "            current_norm_coords = frame_data[:, :2] - np.array([center_x, center_y])\n",
    "            normalized_coords[t] = current_norm_coords\n",
    "            if last_norm_coords is not None:\n",
    "                velocities[t] = current_norm_coords - last_norm_coords\n",
    "            last_norm_coords = current_norm_coords\n",
    "\n",
    "        combined_features = np.concatenate((normalized_coords, velocities), axis=-1)\n",
    "        final_sequence = combined_features.reshape(self.sequence_length, self.input_size)\n",
    "        return final_sequence\n",
    "\n",
    "    def _add_noise(self, sequence_data):\n",
    "        \"\"\"Adds Gaussian noise to the sequence data.\"\"\"\n",
    "        noise = np.random.normal(0, self.noise_level, sequence_data.shape)\n",
    "        return sequence_data + noise.astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_path = self.feature_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            with open(feature_path, 'rb') as f:\n",
    "                sequence_data_raw = pickle.load(f)\n",
    "                # --- Add check for data format ---\n",
    "                if not isinstance(sequence_data_raw, (np.ndarray, torch.Tensor)):\n",
    "                    print(f\"Warning: Loaded data from {feature_path} is not a NumPy array or Tensor (type: {type(sequence_data_raw)}). Attempting conversion.\")\n",
    "                    # Attempt conversion if possible, e.g., from a list of lists\n",
    "                    try:\n",
    "                        sequence_data_raw = np.array(sequence_data_raw, dtype=np.float32)\n",
    "                    except Exception as conv_e:\n",
    "                         print(f\"Error converting data from {feature_path} to NumPy array: {conv_e}. Returning zeros.\")\n",
    "                         return torch.zeros((self.sequence_length, self.input_size), dtype=torch.float32), torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "           \n",
    "            self._normalize_and_calculate_velocity.__globals__['current_path'] = feature_path\n",
    "            processed_sequence = self._normalize_and_calculate_velocity(sequence_data_raw)\n",
    "\n",
    "            if self.is_train and random.random() < self.augment_prob:\n",
    "                processed_sequence = self._add_noise(processed_sequence)\n",
    "            sequence_tensor = torch.tensor(processed_sequence, dtype=torch.float32)\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "            return sequence_tensor, label_tensor\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found {feature_path}.\")\n",
    "            # Return zero tensors matching expected output shape and type\n",
    "            return torch.zeros((self.sequence_length, self.input_size), dtype=torch.float32), torch.tensor(0, dtype=torch.long)\n",
    "        except pickle.UnpicklingError as e:\n",
    "             print(f\"Error unpickling file {feature_path}: {e}. File might be corrupted or empty.\")\n",
    "             return torch.zeros((self.sequence_length, self.input_size), dtype=torch.float32), torch.tensor(0, dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading/processing file {feature_path}: {e}\")\n",
    "            # Return zero tensors matching expected output shape and type\n",
    "            return torch.zeros((self.sequence_length, self.input_size), dtype=torch.float32), torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "# --- Create Datasets ---\n",
    "train_dataset = PoseSequenceDataset(train_files, train_labels, ORIGINAL_SEQUENCE_LENGTH, INPUT_SIZE,\n",
    "                                    orig_landmark_dim=ORIGINAL_LANDMARK_DIM, is_train=True,\n",
    "                                    augment_prob=AUGMENT_PROB, noise_level=NOISE_LEVEL)\n",
    "val_dataset = PoseSequenceDataset(val_files, val_labels, ORIGINAL_SEQUENCE_LENGTH, INPUT_SIZE,\n",
    "                                  orig_landmark_dim=ORIGINAL_LANDMARK_DIM, is_train=False)\n",
    "test_dataset = PoseSequenceDataset(test_files, test_labels, ORIGINAL_SEQUENCE_LENGTH, INPUT_SIZE,\n",
    "                                   orig_landmark_dim=ORIGINAL_LANDMARK_DIM, is_train=False)\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "# Set num_workers=0 on macOS for MPS compatibility if issues arise\n",
    "num_workers = 0 if DEVICE == torch.device('mps') else os.cpu_count() // 2 # Heuristic for num_workers\n",
    "pin_memory = True if DEVICE != torch.device('cpu') else False\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "print(\"\\nDatasets and DataLoaders created with feature engineering & augmentation.\")\n",
    "# --- Quick check of one batch ---\n",
    "try:\n",
    "    # Make sure train_loader is not empty\n",
    "    if len(train_loader) > 0:\n",
    "        seq_batch, label_batch = next(iter(train_loader))\n",
    "        print(f\"Sample batch - Sequence shape: {seq_batch.shape}, Label shape: {label_batch.shape}\")\n",
    "        # Check if batch itself is empty (can happen if last batch is smaller and processing failed)\n",
    "        if seq_batch.nelement() > 0:\n",
    "            print(f\"Sample sequence (first item in batch) - First few features:\\n{seq_batch[0, 0, :10]}...\")\n",
    "            print(f\"Sample labels (first 5 in batch): {label_batch[:5]}\")\n",
    "        else:\n",
    "            print(\"Warning: First batch fetched is empty.\")\n",
    "    else:\n",
    "        print(\"Warning: Train loader has length 0.\")\n",
    "except StopIteration:\n",
    "    print(\"Warning: Train loader is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching batch from train_loader: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Model Building (Bidirectional RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FallDetectionRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, rnn_type='LSTM', dropout_prob=0.2, bidirectional=True):\n",
    "        super(FallDetectionRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        rnn_dropout = dropout_prob if num_layers > 1 else 0\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                               batch_first=True, dropout=rnn_dropout,\n",
    "                               bidirectional=bidirectional)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                              batch_first=True, dropout=rnn_dropout,\n",
    "                              bidirectional=bidirectional)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Corrected: Ensure fc layer input size matches RNN output size\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input tensor is on the correct device\n",
    "        x = x.to(next(self.parameters()).device) # Ensure input is on same device as model\n",
    "\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(x.device)\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            c0 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(x.device)\n",
    "            hidden = (h0, c0)\n",
    "        else: # GRU\n",
    "            hidden = h0\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        # 'out' contains outputs for every time step\n",
    "        # '_' contains the final hidden state (h_n, c_n for LSTM)\n",
    "        out, _ = self.rnn(x, hidden)\n",
    "\n",
    "        # We typically use the output of the *last* time step for classification\n",
    "        # out shape: (batch_size, seq_length, hidden_size * num_directions)\n",
    "        # Select the output of the last time step (-1)\n",
    "        last_step_out = out[:, -1, :]\n",
    "\n",
    "        # Apply dropout and pass through the fully connected layer\n",
    "        last_step_out = self.dropout(last_step_out)\n",
    "        out = self.fc(last_step_out)\n",
    "        return out\n",
    "\n",
    "# --- Instantiate the model ---\n",
    "# Note: DROPOUT_PROB from Cell 1 is passed here\n",
    "model = FallDetectionRNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES,\n",
    "                         rnn_type=RNN_TYPE, dropout_prob=DROPOUT_PROB,\n",
    "                         bidirectional=BIDIRECTIONAL)\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(\"\\nModel Architecture (v3 - Weighted Loss Attempt):\")\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Training Setup - Loss Function (Potentially Weighted), Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Class Weights (if enabled) ---\n",
    "class_weights_tensor = None\n",
    "if USE_WEIGHTED_LOSS:\n",
    "    print(\"\\nCalculating class weights for weighted loss...\")\n",
    "    # Count occurrences of each class *only in the training set*\n",
    "    label_counts = Counter(train_labels)\n",
    "    if len(label_counts) != NUM_CLASSES:\n",
    "        print(f\"Warning: Training set does not contain all {NUM_CLASSES} classes! Found {len(label_counts)}.\")\n",
    "        # Ensure all classes potentially have a count, even if 0\n",
    "        for i in range(NUM_CLASSES):\n",
    "            label_counts[i] = label_counts.get(i, 0) # Default to 0 if not present\n",
    "\n",
    "    # Calculate weights - inverse frequency based formula\n",
    "    total_samples = len(train_labels)\n",
    "    weights = []\n",
    "    # Ensure weights are calculated in the correct order (0, 1, 2, ...)\n",
    "    for i in range(NUM_CLASSES):\n",
    "        count = label_counts.get(i, 0) # Get count, default to 0 if somehow missed\n",
    "        if count == 0:\n",
    "            # Handle missing class: Assign a high weight or a default weight?\n",
    "            # Using total_samples / NUM_CLASSES as a proxy for average count if one class had 1 sample.\n",
    "            # This gives it significant weight but avoids division by zero.\n",
    "            # You might adjust this strategy based on domain knowledge.\n",
    "            print(f\"Warning: Class {index_to_name.get(i, 'Unknown')} (index {i}) has 0 samples in training set. Assigning calculated high weight.\")\n",
    "            # Avoid division by zero if NUM_CLASSES is 0 or total_samples is 0 (although unlikely here)\n",
    "            weight = (total_samples / NUM_CLASSES) if NUM_CLASSES > 0 and total_samples > 0 else 1.0\n",
    "        else:\n",
    "            # Common formula: total_samples / (num_classes * count)\n",
    "            # Add a small epsilon to prevent division by zero if NUM_CLASSES is somehow 0\n",
    "             weight = total_samples / ((NUM_CLASSES * count) + 1e-6)\n",
    "        weights.append(weight)\n",
    "\n",
    "    class_weights_tensor = torch.tensor(weights, dtype=torch.float32).to(DEVICE)\n",
    "    print(f\"  Class weights calculated: {class_weights_tensor.cpu().numpy()}\")\n",
    "    # Map weights back to class names for clarity\n",
    "    class_weight_dict = {index_to_name.get(i, f'Unknown_{i}'): w for i, w in enumerate(weights)} # Added default name\n",
    "    print(f\"  Weights per class: {class_weight_dict}\")\n",
    "\n",
    "# --- Define Loss Function ---\n",
    "# Use weights if calculated, otherwise use standard CE Loss\n",
    "if class_weights_tensor is not None and USE_WEIGHTED_LOSS: # Check flag again just in case\n",
    "     criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "     print(\"  Using Weighted CrossEntropyLoss.\")\n",
    "else:\n",
    "     criterion = nn.CrossEntropyLoss()\n",
    "     print(\"  Using standard CrossEntropyLoss.\")\n",
    "\n",
    "# --- Define Optimizer and Scheduler ---\n",
    "# Note: LEARNING_RATE and WEIGHT_DECAY from Cell 1 are used here\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "# Note: PATIENCE_LR_SCHEDULER from Cell 1 is used here\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=PATIENCE_LR_SCHEDULER, verbose=True)\n",
    "\n",
    "print(\"\\nLoss function and optimizer defined.\")\n",
    "print(f\"  Criterion: {type(criterion).__name__}\")\n",
    "print(f\"  Optimizer: AdamW (LR={LEARNING_RATE}, Weight Decay={WEIGHT_DECAY})\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau (Patience={PATIENCE_LR_SCHEDULER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Training & Validation Loop - Includes Early Stopping & Accuracy Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluates the model on a given dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0; correct_preds = 0; total_samples = 0\n",
    "\n",
    "    # Check if dataloader is empty before iterating\n",
    "    if not dataloader:\n",
    "        print(\"Warning: Dataloader is empty in evaluate_model.\")\n",
    "        return 0.0, 0.0\n",
    "    if len(dataloader) == 0:\n",
    "        print(\"Warning: Dataloader has length 0 in evaluate_model.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in dataloader:\n",
    "            # Added check for empty batch inside the loop as well\n",
    "            if sequences.nelement() == 0 or labels.nelement() == 0:\n",
    "                 print(\"Warning: Skipping empty batch during evaluation.\")\n",
    "                 continue\n",
    "\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            try:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels) # Criterion will use weights if defined\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_preds += (predicted == labels).sum().item()\n",
    "            except Exception as eval_e:\n",
    "                print(f\"Error during evaluation batch: {eval_e}\")\n",
    "                print(f\"  Sequence shape: {sequences.shape}, Labels: {labels}\")\n",
    "                # Optionally skip this batch or re-raise\n",
    "                continue # Skip batch\n",
    "\n",
    "    # Calculate metrics, avoid division by zero\n",
    "    # Use len(dataloader.dataset) if available and more robust, otherwise len(dataloader) * batch_size approx\n",
    "    num_batches = len(dataloader)\n",
    "    epoch_loss = running_loss / num_batches if num_batches > 0 else 0.0\n",
    "    epoch_acc = (100.0 * correct_preds / total_samples) if total_samples > 0 else 0.0\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"\\nStarting Training (v3 - Weighted Loss Attempt)...\")\n",
    "best_val_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# <<< Initialize lists to store accuracy history >>>\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = [] # Also track loss\n",
    "val_losses = []   # Also track loss\n",
    "\n",
    "# Use the new model name defined in Cell 1\n",
    "model_save_path = os.path.join(MODEL_SAVE_DIR, MODEL_NAME)\n",
    "\n",
    "# --- Training Loop ---\n",
    "epochs_completed = 0 # Track actual epochs run\n",
    "if len(train_loader) == 0:\n",
    "     print(\"Error: Training loader is empty. Cannot start training.\")\n",
    "else:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss_train = 0.0; correct_train = 0; total_train = 0\n",
    "        batches_processed_train = 0\n",
    "\n",
    "        for i, (sequences, labels) in enumerate(train_loader):\n",
    "\n",
    "            # --- Defensive Check --- Check for empty batch\n",
    "            if sequences.nelement() == 0:\n",
    "                 print(f\"Warning: Empty sequence tensor encountered in training epoch {epoch+1}, batch {i}. Skipping.\")\n",
    "                 continue\n",
    "            if labels.nelement() == 0:\n",
    "                 print(f\"Warning: Empty label tensor encountered in training epoch {epoch+1}, batch {i}. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels) # Criterion will use weights if defined\n",
    "\n",
    "                # --- Check for NaN/Inf loss ---\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"Warning: NaN or Inf loss detected at epoch {epoch+1}, batch {i}. Skipping update.\")\n",
    "                    # Consider logging inputs/outputs here for debugging\n",
    "                    continue # Skip backprop and step for this batch\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # Optional: Gradient Clipping (helps prevent exploding gradients)\n",
    "                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss_train += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                batches_processed_train += 1\n",
    "\n",
    "            except Exception as train_e:\n",
    "                print(f\"Error during training batch {i} in epoch {epoch+1}: {train_e}\")\n",
    "                print(f\"  Sequence shape: {sequences.shape}, Labels: {labels}\")\n",
    "                # Decide whether to continue to next batch or stop\n",
    "                continue # Continue to next batch\n",
    "\n",
    "        # --- Calculate Epoch Metrics ---\n",
    "        # Avoid division by zero if no batches were processed successfully\n",
    "        train_loss = running_loss_train / batches_processed_train if batches_processed_train > 0 else 0.0\n",
    "        train_accuracy = (100.0 * correct_train / total_train) if total_train > 0 else 0.0\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epochs_completed += 1 # Increment actual epochs completed\n",
    "\n",
    "        # <<< Store metrics for plotting >>>\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_duration:.2f}s) | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}% | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # --- LR Scheduling & Early Stopping --- based on Validation Accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            try:\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"  -> Val accuracy improved to {best_val_accuracy:.2f}%. Saving model to {model_save_path}\")\n",
    "            except Exception as save_e:\n",
    "                 print(f\"  -> Val accuracy improved to {best_val_accuracy:.2f}%, BUT FAILED TO SAVE MODEL: {save_e}\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  -> Val accuracy did not improve for {epochs_no_improve} epoch(s). Best: {best_val_accuracy:.2f}% (Patience: {PATIENCE_EARLY_STOPPING})\")\n",
    "\n",
    "        # Note: PATIENCE_EARLY_STOPPING from Cell 1 is used here\n",
    "        if epochs_no_improve >= PATIENCE_EARLY_STOPPING:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs due to lack of improvement in validation accuracy.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nTraining Finished after {epochs_completed} epochs.\")\n",
    "    if os.path.exists(model_save_path):\n",
    "         print(f\"Best model (val acc: {best_val_accuracy:.2f}%) saved: {model_save_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: No model saved at {model_save_path}. Training may have failed, stopped early before improvement, or save failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Final Testing & Evaluation (with Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting Final Testing on the Unseen Test Set...\")\n",
    "\n",
    "# Check if a model was actually saved (or training completed)\n",
    "if 'model_save_path' in locals() and os.path.exists(model_save_path):\n",
    "    print(f\"Loading best model from: {model_save_path}\")\n",
    "    # Instantiate a fresh model instance with the *same* parameters as the saved one\n",
    "    final_model = FallDetectionRNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES,\n",
    "                                   rnn_type=RNN_TYPE, dropout_prob=DROPOUT_PROB, # Use dropout from config\n",
    "                                   bidirectional=BIDIRECTIONAL)\n",
    "    try:\n",
    "        # Determine map_location based on the device used for training/evaluation\n",
    "        map_location = torch.device('cpu') if DEVICE == torch.device('cpu') else DEVICE\n",
    "        final_model.load_state_dict(torch.load(model_save_path, map_location=map_location))\n",
    "        final_model.to(DEVICE)\n",
    "        final_model.eval() # Set to evaluation mode\n",
    "\n",
    "        # --- Evaluate on Test Set ---\n",
    "        test_loss, test_accuracy = evaluate_model(final_model, test_loader, criterion, DEVICE) # Use same criterion as training\n",
    "\n",
    "        print(\"\\n--- Test Set Evaluation Results ---\")\n",
    "        print(f\"  Test Loss: {test_loss:.4f} (using {'weighted' if USE_WEIGHTED_LOSS and class_weights_tensor is not None else 'standard'} criterion)\")\n",
    "        print(f\"  Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "        # --- Generate Classification Report and Confusion Matrix ---\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        with torch.no_grad():\n",
    "            if len(test_loader) == 0:\n",
    "                 print(\"Warning: Test loader is empty. Cannot generate report/matrix.\")\n",
    "            else:\n",
    "                for sequences, labels in test_loader:\n",
    "                    # <<< Add check for empty batch in test loader >>>\n",
    "                    if sequences.nelement() == 0 or labels.nelement() == 0:\n",
    "                        print(\"Warning: Skipping empty batch in test loader during final evaluation.\")\n",
    "                        continue\n",
    "\n",
    "                    sequences = sequences.to(DEVICE)\n",
    "                    # Ensure model processes correctly\n",
    "                    try:\n",
    "                        outputs = final_model(sequences)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        all_preds.extend(predicted.cpu().numpy()) # Move predictions to CPU\n",
    "                        all_true.extend(labels.cpu().numpy()) # Move true labels to CPU\n",
    "                    except Exception as test_eval_e:\n",
    "                        print(f\"Error evaluating batch in test set: {test_eval_e}\")\n",
    "                        print(f\"  Sequence shape: {sequences.shape}\")\n",
    "                        continue # Skip this batch\n",
    "\n",
    "        # Check if any predictions were made\n",
    "        if not all_preds or not all_true:\n",
    "             print(\"Error: No predictions generated for the test set (loader might be empty or all batches failed). Cannot create report or matrix.\")\n",
    "        else:\n",
    "            print(\"\\n--- Classification Report (Test Set) ---\")\n",
    "            # Ensure target_names uses the CLASS_FOLDERS from Cell 1\n",
    "            # Make sure CLASS_FOLDERS list indices align with label_map values\n",
    "            target_names_report = [index_to_name.get(i, f'Class_{i}') for i in sorted(index_to_name.keys())]\n",
    "            print(classification_report(all_true, all_preds, labels=sorted(index_to_name.keys()), target_names=target_names_report, digits=3, zero_division=0))\n",
    "\n",
    "            print(\"\\n--- Confusion Matrix (Test Set) ---\")\n",
    "            cm = confusion_matrix(all_true, all_preds, labels=sorted(index_to_name.keys())) # Ensure labels are correctly ordered\n",
    "            # Ensure index/columns use the index_to_name mapping from Cell 1\n",
    "            # Handle potential missing classes in predictions/true labels for CM indexing\n",
    "            cm_labels_names = [index_to_name.get(i, f\"Class_{i}\") for i in sorted(index_to_name.keys())]\n",
    "            cm_df = pd.DataFrame(cm, index=cm_labels_names, columns=cm_labels_names)\n",
    "\n",
    "            plt.figure(figsize=(9, 7))\n",
    "            sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', xticklabels=cm_labels_names, yticklabels=cm_labels_names)\n",
    "            plt.title('Confusion Matrix (Test Set - v3 Weighted Loss Attempt)') # Updated title\n",
    "            plt.ylabel('Actual Class')\n",
    "            plt.xlabel('Predicted Class')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot using the new model name base\n",
    "            cm_save_path = os.path.join(MODEL_SAVE_DIR, f'{os.path.splitext(MODEL_NAME)[0]}_confusion_matrix.png')\n",
    "            try:\n",
    "                plt.savefig(cm_save_path)\n",
    "                print(f\"Confusion matrix saved to: {cm_save_path}\")\n",
    "            except Exception as plot_e:\n",
    "                print(f\"Error saving confusion matrix plot: {plot_e}\")\n",
    "            plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "         print(f\"Error: Model file not found at {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or evaluating on test set: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "else:\n",
    "    print(f\"Could not find the saved model file (expected at {model_save_path if 'model_save_path' in locals() else 'path not defined'}). Skipping final testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Plot Training & Validation Accuracy/Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPlotting Training & Validation Accuracy/Loss...\")\n",
    "\n",
    "# Check if training actually ran and recorded metrics\n",
    "# Need to ensure these lists exist and are not empty\n",
    "if ('train_accuracies' in locals() and train_accuracies and\n",
    "    'val_accuracies' in locals() and val_accuracies and\n",
    "    'epochs_completed' in locals() and epochs_completed > 0):\n",
    "\n",
    "    epochs_range = range(1, epochs_completed + 1) # Use actual completed epochs\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1) # Create subplot for accuracy\n",
    "    plt.plot(epochs_range, train_accuracies, label='Training Accuracy', marker='o', linestyle='-')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy', marker='x', linestyle='--')\n",
    "    plt.title(f'Training & Validation Accuracy ({os.path.splitext(MODEL_NAME)[0]})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 105) # Set Y-axis limits for accuracy (0-100%)\n",
    "\n",
    "    # Plot Loss (Optional but good practice)\n",
    "    if 'train_losses' in locals() and train_losses and 'val_losses' in locals() and val_losses:\n",
    "        plt.subplot(1, 2, 2) # Create subplot for loss\n",
    "        plt.plot(epochs_range, train_losses, label='Training Loss', marker='o', linestyle='-')\n",
    "        plt.plot(epochs_range, val_losses, label='Validation Loss', marker='x', linestyle='--')\n",
    "        plt.title(f'Training & Validation Loss ({os.path.splitext(MODEL_NAME)[0]})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        # Adjust Y-axis for loss if needed, e.g., plt.ylim(bottom=0)\n",
    "\n",
    "    plt.tight_layout() # Adjust subplot params for a tight layout\n",
    "\n",
    "    # Save the combined plot\n",
    "    acc_loss_plot_save_path = os.path.join(MODEL_SAVE_DIR, f'{os.path.splitext(MODEL_NAME)[0]}_accuracy_loss_plot.png')\n",
    "    try:\n",
    "        plt.savefig(acc_loss_plot_save_path)\n",
    "        print(f\"Accuracy and Loss plot saved to: {acc_loss_plot_save_path}\")\n",
    "    except Exception as plot_e:\n",
    "        print(f\"Error saving accuracy/loss plot: {plot_e}\")\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No accuracy/loss data recorded (training might have failed early, stopped immediately, or lists are empty). Skipping plot generation.\")\n",
    "\n",
    "print(\"\\n--- End of Script (v3 - Weighted Loss Attempt) ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
